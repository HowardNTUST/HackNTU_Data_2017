{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run magic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning for classification\n",
    "給一堆 $x$, 和他的分類，我們找出計算 x 的分類的方式\n",
    "\n",
    "### One hot encoding\n",
    "如果我們有三類種類別， 我們可以來編碼這三個類別\n",
    "* $(1,0,0)$\n",
    "* $(0,1,0)$\n",
    "* $(0,0,1)$\n",
    "\n",
    "### 問題\n",
    "* 為什麼不直接用 1,2,3 這樣的編碼呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression 的模型是這樣的\n",
    "我們的輸入 $x=\\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} $ 是一個向量，我們看成 column vector 好了\n",
    "\n",
    "而 Weight: $W = \\begin{pmatrix} W_0 \\\\ W_1 \\\\ W_2 \\end{pmatrix} =  \n",
    "\\begin{pmatrix} W_{0,0} & W_{0,1} &  W_{0,2} & W_{0,3}\\\\ \n",
    " W_{1,0} & W_{1,1} &  W_{1,2} & W_{1,3} \\\\ \n",
    " W_{2,0} & W_{2,1} &  W_{2,2} & W_{2,3} \\end{pmatrix} $\n",
    " \n",
    " Bias: $b=\\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{pmatrix} $ \n",
    "\n",
    "\n",
    "我們先計算\"線性輸出\"  $ c = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\end{pmatrix} =  Wx+b =\n",
    "\\begin{pmatrix} W_0 x + b_0 \\\\ W_1 x + b_1 \\\\ W_2 x + b_2 \\end{pmatrix}   $， 然後再取 $exp$ (逐項取)。 最後得到一個向量。\n",
    " \n",
    " $d = \\begin{pmatrix} d_0 \\\\ d_1 \\\\ d_2 \\end{pmatrix} = e^{W x + b} = \\begin{pmatrix} e^{c_0} \\\\ e^{c_1} \\\\ e^{c_2} \\end{pmatrix}$\n",
    "\n",
    "\n",
    "將這些數值除以他們的總和。\n",
    "我們希望出來的數字會符合這張圖片是這個數字的條件機率。\n",
    "\n",
    "###  $q(i) = Predict_{W,b}(Y=i|x)  = \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}} = \\frac {d_i} {\\sum_j d_j}$\n",
    "\n",
    "### 問題\n",
    "* 為什麼要用 $exp$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先隨便算一個 $\\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ 的網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000&2.000\\\\ 3.000&4.000\\\\ 5.000&6.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 3.,  4.],\n",
       "       [ 5.,  6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight\n",
    "W = Matrix([1,2],[3,4], [5,6])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias\n",
    "b = Vector(1,0,-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}2.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 2.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸入\n",
    "x = Vector(2,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務：計算最後的猜測機率 $q$\n",
    "Hint: `np.exp` 可以算 $exp$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "# Wx+b\n",
    "c = W @ x + b\n",
    "\n",
    "# d = exp(Wx+b)\n",
    "d = np.exp(c)\n",
    "\n",
    "# q = d/sum(d)\n",
    "q = d/d.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 4 的餘數分成四類\n",
    "\n",
    "Hint: 可以參考上面 W, b 的設定方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ 1.000\\\\ 1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint 下面產生數字 i 的 2 進位向量\n",
    "i = 13\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "W = Matrix([-1,-1,0,0], [1,-1,0,0], [-1,1,0,0], [1,1,0,0])\n",
    "b = Vector(0,0,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    r = W @ x + b\n",
    "    print(\"i=\", i, \"predict:\", r.argmax(), \"ground truth:\", i%4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 3 的餘數分成三類\n",
    "\n",
    "Hint: 不用全部正確，用猜的，但正確率要比亂猜高。可以利用統計的結果猜猜看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "W = Matrix([0,0,0,0], [1,-1,1,-1], [-1,1,-1,1])\n",
    "b = Vector(0.1,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    r = W @ x + b\n",
    "    print(\"i=\", i, \"predict:\", r.argmax(), \"ground truth:\", i%3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差函數\n",
    "為了要評斷我們的預測的品質，要設計一個評斷誤差的方式\n",
    "\n",
    "假設輸入值 $x$ 對應到的真實類別是 $y$, 那我們定義誤差函數\n",
    "\n",
    "## $ loss = -\\log(q(y))=- \\log(Predict(Y=y|x, W,b)) $\n",
    "\n",
    "\n",
    "其實比較一般但比較複雜一點的寫法是\n",
    "\n",
    "## $ loss = - \\sum_i p(i)\\log(q(i))  $\n",
    "\n",
    "其中 $i$ 是所有類別， 而 $ p(i) = \\Pr(Y=i|x) $ 是真實發生的機率\n",
    "\n",
    "但我們目前 $x$ 對應到的真實類別是 $y$， 所以直接 $p(y)=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 想辦法改進。 \n",
    "我們用一種被稱作是 gradient descent 的方式來改善我們的誤差。\n",
    "\n",
    "因為我們知道 gradient 是讓函數上升最快的方向。所以我們如果朝 gradient 的反方向走一點點（也就是下降最快的方向），那麼得到的函數值應該會小一點。\n",
    "\n",
    "記得我們的變數是 $W$ 和 $b$ (裡面有一堆 W_i,j b_i 這些變數)，所以我們要把 $loss$ 對 $W$ 和 $b$ 裡面的每一個參數來偏微分。\n",
    "\n",
    "還好這個偏微分是可以用手算出他的形式，而最後偏微分的式子也不會很複雜。\n",
    "\n",
    "$loss$ 展開後可以寫成\n",
    "## $loss = -\\log(q(y)) = \\log(\\sum_j d_j) - d_i \\\\\n",
    " = \\log(\\sum_j e^{W_j x + b_j}) - W_i x - b_i$\n",
    "\n",
    "注意 $d_j = e^{W_j x + b_j}$ 只有變數 $b_j, W_j$ \n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k}}{\\sum_j e^{W_j x + b_j}} = q(k)$$\n",
    "對 $k = i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    "$$ q(k) - 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對 $W$ 的偏微分也不難\n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k} W_{k,t} x_t}{\\sum_j e^{W_j x + b_j}} = q(k) x_t$$\n",
    "對 $k = i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    "$$ q(k) x_t - x_t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實做部份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先產生隨機的 W 和 b\n",
    "W = Matrix(np.random.normal(size=(3,4)))\n",
    "b = Vector(np.random.normal(size=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-0.710&0.909&-0.185&-0.453\\\\ 0.454&0.516&-1.102&-0.025\\\\ 0.126&0.497&-0.533&0.929\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-0.71007037,  0.90924923, -0.18450098, -0.452941  ],\n",
       "       [ 0.45422787,  0.51598233, -1.10175927, -0.02488268],\n",
       "       [ 0.12595801,  0.49698172, -0.53335558,  0.92919945]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.428\\\\ 0.911\\\\ -0.335\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.42784862],\n",
       "       [ 0.91115569],\n",
       "       [-0.33484191]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    "W, b 的 size 為什麼要這樣設定？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務： 隨便設定一組 x, y, 我們來跑跑看 gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 14\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "y = i%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算 q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "# Wx+b\n",
    "c = W @ x + b\n",
    "\n",
    "# d = exp(Wx+b)\n",
    "d = np.exp(c)\n",
    "\n",
    "# q = d/sum(d)\n",
    "q = d/d.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟： 計算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "loss = -np.log(q[y])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算對 b 的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.394\\\\ 0.264\\\\ -0.658\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.39388855],\n",
       "       [ 0.26425332],\n",
       "       [-0.65814187]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#參考答案\n",
    "%run -i  softmax_compute_grad_b.py\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算對 W 的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "grad_W =   q @ x.T\n",
    "grad_W[y] -= x.ravel()\n",
    "\n",
    "# or \n",
    "grad_W = grad_b @ x.T\n",
    "\n",
    "grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：更新 W, b  各減掉 0.5 * gradient， 然後看看新的 loss 是否有進步了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "%run -i softmax_update_Wb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.394\\\\ 0.264\\\\ 0.342\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.39388855],\n",
       "       [ 0.26425332],\n",
       "       [ 0.34185813]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原先的 q\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.073\\end{pmatrix}"
      ],
      "text/plain": [
       "array([ 1.07335947])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原先的 loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.233\\end{pmatrix}"
      ],
      "text/plain": [
       "array([ 0.2332524])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 現在的 loss\n",
    "%run -i softmax_compute_q.py\n",
    "%run -i softmax_compute_loss1.py\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.111\\\\ 0.097\\\\ 0.792\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.1112872 ],\n",
       "       [ 0.09675915],\n",
       "       [ 0.79195366]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次訓練多組資料\n",
    "上面只針對一組 x (i=14) 來訓練，如果一次對所有 x 訓練呢？\n",
    "\n",
    "通常我們會把組別放在 axis-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000\\\\ 0.000\\\\ 0.000\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ 0.000\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 2\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000\\\\ 1.000\\\\ 0.000\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 3\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 1.000\\\\ 0.000\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2) for i in range(16)])\n",
    "for i in range(4):\n",
    "    print(\"i=\", i)\n",
    "    display(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{bmatrix}[0.000&0.000&0.000&0.000]\\\\ [1.000&0.000&0.000&0.000]\\\\ [0.000&1.000&0.000&0.000]\\\\ [1.000&1.000&0.000&0.000]\\\\ [0.000&0.000&1.000&0.000]\\\\ [1.000&0.000&1.000&0.000]\\\\ [0.000&1.000&1.000&0.000]\\\\ [1.000&1.000&1.000&0.000]\\\\ [0.000&0.000&0.000&1.000]\\\\ [1.000&0.000&0.000&1.000]\\\\ [0.000&1.000&0.000&1.000]\\\\ [1.000&1.000&0.000&1.000]\\\\ [0.000&0.000&1.000&1.000]\\\\ [1.000&0.000&1.000&1.000]\\\\ [0.000&1.000&1.000&1.000]\\\\ [1.000&1.000&1.000&1.000]\\end{bmatrix}"
      ],
      "text/plain": [
       "array([[[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 0.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]],\n",
       "\n",
       "       [[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 對應的組別 \n",
    "y = np.array([i%3 for i in range(16)])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務： 將訓練向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 參考解答如後"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "d = np.exp(W @ x + b)\n",
    "q = d/d.sum()\n",
    "q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{bmatrix}[0.284&0.492&0.224]\\\\ [0.120&0.663&0.218]\\\\ [0.319&0.398&0.283]\\\\ [0.142&0.568&0.290]\\\\ [0.373&0.275&0.352]\\\\ [0.181&0.427&0.393]\\\\ [0.387&0.205&0.408]\\\\ [0.195&0.331&0.474]\\\\ [0.109&0.309&0.581]\\\\ [0.045&0.406&0.549]\\\\ [0.111&0.227&0.662]\\\\ [0.047&0.307&0.646]\\\\ [0.117&0.141&0.742]\\\\ [0.051&0.198&0.751]\\\\ [0.111&0.097&0.792]\\\\ [0.049&0.138&0.813]\\end{bmatrix}"
      ],
      "text/plain": [
       "array([[[ 0.2841512 ],\n",
       "        [ 0.49158318],\n",
       "        [ 0.22426562]],\n",
       "\n",
       "       [[ 0.11956982],\n",
       "        [ 0.66270067],\n",
       "        [ 0.21772951]],\n",
       "\n",
       "       [[ 0.31948882],\n",
       "        [ 0.39797838],\n",
       "        [ 0.2825328 ]],\n",
       "\n",
       "       [[ 0.14222655],\n",
       "        [ 0.56758747],\n",
       "        [ 0.29018597]],\n",
       "\n",
       "       [[ 0.37315529],\n",
       "        [ 0.2752497 ],\n",
       "        [ 0.35159501]],\n",
       "\n",
       "       [[ 0.18060325],\n",
       "        [ 0.42678693],\n",
       "        [ 0.39260982]],\n",
       "\n",
       "       [[ 0.3865703 ],\n",
       "        [ 0.20531563],\n",
       "        [ 0.40811407]],\n",
       "\n",
       "       [[ 0.19465459],\n",
       "        [ 0.33121232],\n",
       "        [ 0.47413308]],\n",
       "\n",
       "       [[ 0.1092647 ],\n",
       "        [ 0.30944302],\n",
       "        [ 0.58129228]],\n",
       "\n",
       "       [[ 0.04474818],\n",
       "        [ 0.4059986 ],\n",
       "        [ 0.54925322]],\n",
       "\n",
       "       [[ 0.11110955],\n",
       "        [ 0.22657318],\n",
       "        [ 0.66231727]],\n",
       "\n",
       "       [[ 0.04697949],\n",
       "        [ 0.30691191],\n",
       "        [ 0.6461086 ]],\n",
       "\n",
       "       [[ 0.11684029],\n",
       "        [ 0.14108576],\n",
       "        [ 0.74207396]],\n",
       "\n",
       "       [[ 0.05122473],\n",
       "        [ 0.19816115],\n",
       "        [ 0.75061413]],\n",
       "\n",
       "       [[ 0.1112872 ],\n",
       "        [ 0.09675915],\n",
       "        [ 0.79195366]],\n",
       "\n",
       "       [[ 0.04949494],\n",
       "        [ 0.13786561],\n",
       "        [ 0.81263945]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.exp(W @ X + b)\n",
    "q = d/d.sum(axis=(1,2), keepdims=True)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "loss = -np.log(q[y])\n",
    "loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.258\\\\ 0.411\\\\ 1.264\\\\ 1.950\\\\ 1.290\\\\ 0.935\\\\ 0.950\\\\ 1.105\\\\ 0.543\\\\ 3.107\\\\ 1.485\\\\ 0.437\\\\ 2.147\\\\ 1.619\\\\ 0.233\\\\ 3.006\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.2582488 ],\n",
       "       [ 0.41143188],\n",
       "       [ 1.26396061],\n",
       "       [ 1.95033404],\n",
       "       [ 1.2900766 ],\n",
       "       [ 0.93493897],\n",
       "       [ 0.95044155],\n",
       "       [ 1.10499565],\n",
       "       [ 0.54250158],\n",
       "       [ 3.10670443],\n",
       "       [ 1.4846873 ],\n",
       "       [ 0.43678768],\n",
       "       [ 2.14694733],\n",
       "       [ 1.61867471],\n",
       "       [ 0.2332524 ],\n",
       "       [ 3.00588479]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.log(q[range(len(y)), y])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3587417698356048"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用平均當成我們真正的 loss\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "grad_b = q - np.eye(3)[y][:, None]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-0.210\\\\ 0.011\\\\ 0.199\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-0.20991444],\n",
       "       [ 0.01132579],\n",
       "       [ 0.19858865]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fancy indexing :p\n",
    "one_y = np.eye(3)[y][..., None]\n",
    "grad_b_all = q - one_y\n",
    "grad_b = grad_b_all.mean(axis=0)\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "grad_W = grad_b @ x.T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-0.136&-0.102&-0.096&-0.147\\\\ 0.002&0.017&-0.074&-0.011\\\\ 0.133&0.085&0.170&0.159\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-0.13565615, -0.10238678, -0.09601059, -0.14744068],\n",
       "       [ 0.00232654,  0.01688773, -0.07422274, -0.0110751 ],\n",
       "       [ 0.13332961,  0.08549906,  0.17023332,  0.15851578]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W_all = grad_b_all @ X.swapaxes(1,2)\n",
    "grad_W = grad_W_all.mean(axis=0)\n",
    "grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W -=  0.5 * grad_W\n",
    "b -=  0.5 * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3587417698356048"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 之前的 loss\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2581382935567409"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.exp(W @ X + b)\n",
    "q = d/d.sum(axis=(1,2), keepdims=True)\n",
    "loss = -np.log(q[range(len(y)), y])\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 任務：全部合在一起\n",
    "* 設定 W,b\n",
    "* 設定 X\n",
    "* 訓練三十次\n",
    "    * 計算 q 和 loss    \n",
    "    * 計算 grad_b 和 grad_W\n",
    "    * 更新 W, b\n",
    "* 看看準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在這裡計算\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3125\n",
      "1 0.3125\n",
      "2 0.25\n",
      "3 0.3125\n",
      "4 0.4375\n",
      "5 0.4375\n",
      "6 0.4375\n",
      "7 0.5\n",
      "8 0.5\n",
      "9 0.5625\n",
      "10 0.5625\n",
      "11 0.5625\n",
      "12 0.5625\n",
      "13 0.5625\n",
      "14 0.625\n",
      "15 0.625\n",
      "16 0.625\n",
      "17 0.625\n",
      "18 0.6875\n",
      "19 0.6875\n",
      "20 0.6875\n",
      "21 0.6875\n",
      "22 0.6875\n",
      "23 0.6875\n",
      "24 0.6875\n",
      "25 0.6875\n",
      "26 0.6875\n",
      "27 0.6875\n",
      "28 0.6875\n",
      "29 0.6875\n",
      "30 0.6875\n",
      "31 0.6875\n",
      "32 0.75\n",
      "33 0.8125\n",
      "34 0.8125\n",
      "35 0.8125\n",
      "36 0.875\n",
      "37 0.875\n",
      "38 0.875\n",
      "39 0.875\n",
      "40 0.875\n",
      "41 0.875\n",
      "42 0.875\n",
      "43 0.875\n",
      "44 0.875\n",
      "45 0.875\n",
      "46 0.875\n",
      "47 0.875\n",
      "48 0.875\n",
      "49 0.875\n"
     ]
    }
   ],
   "source": [
    "# 參考答案\n",
    "%run -i softmax_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgp3EZEQUriLiISQwl1EJIT+P2suT9JfslXMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe944e94438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 畫出 loss 的曲線\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000&1.000&2.000&0.000&1.000&1.000&0.000&1.000&2.000&0.000&2.000&2.000&0.000&1.000&2.000&0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([0, 1, 2, 0, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000&1.000&2.000&0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 對答案\n",
    "display((W @ X + b).argmax(axis=1).ravel())\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
